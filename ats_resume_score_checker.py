# -*- coding: utf-8 -*-
"""ATS Resume Score Checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGVluoe0r-N58TYi7q1iMhdkjGI5tKZn
"""

!pip install python-docx PyPDF2 nltk scikit-learn matplotlib spacy --quiet
!python -m spacy download en_core_web_sm --quiet

import re
import nltk
import spacy
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import matplotlib.pyplot as plt
from google.colab import files
import ipywidgets as widgets
from IPython.display import display, HTML
import time

nlp = spacy.load("en_core_web_sm")
nltk.download('stopwords')
nltk.download('punkt')

def extract_text(file_path):
    text = ""
    try:
        if file_path.lower().endswith('.pdf'):
            import PyPDF2
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = "\n".join([page.extract_text() or "" for page in reader.pages])
        elif file_path.lower().endswith('.docx'):
            import docx
            doc = docx.Document(file_path)
            text = "\n".join([para.text for para in doc.paragraphs])
        else:
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
        return text.strip()
    except Exception as e:
        print(f"Error reading file: {str(e)}")
        return ""

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    doc = nlp(text)
    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

class ATSScanner:
    def __init__(self):
        self.jd_keywords = []
        self.jd_text = ""

    def calculate_base_score(self, resume_text):
        score = 0

        sections = {
            'experience': 15,
            'education': 10,
            'skills': 10,
            'projects': 5
        }

        present_sections = []
        for section, points in sections.items():
            if re.search(rf'\b{section}\b', resume_text.lower()):
                score += points
                present_sections.append(section)

        if any(char in resume_text for char in ['â€¢', '-', '*']): score += 5
        if len(resume_text.split()) <= 800: score += 10
        if any(verb in resume_text.lower() for verb in ['led', 'managed', 'achieved']): score += 5

        return score, present_sections

    def calculate_jd_match(self, resume_text, jd_text):
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([resume_text, jd_text])
        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]

    def extract_jd_keywords(self, jd_text):
        doc = nlp(jd_text.lower())
        return [token.lemma_ for token in doc if not token.is_stop
                and not token.is_punct
                and token.pos_ in ['NOUN', 'PROPN', 'VERB']]

    def analyze(self, resume_text, jd_text=None):
        base_score, sections = self.calculate_base_score(resume_text)

        if jd_text:
            self.jd_text = jd_text
            self.jd_keywords = self.extract_jd_keywords(jd_text)
            jd_match = self.calculate_jd_match(resume_text, jd_text)
            final_score = base_score + (jd_match * 40)
        else:
            final_score = base_score * 1.4

        return min(100, int(final_score)), sections

class ResumeAnalyzer:
    def __init__(self):
        self.progress = widgets.FloatProgress(value=0, max=100, description='Progress:')
        self.status = widgets.HTML()
        self.score_display = widgets.Output()
        self.improvement_display = widgets.Output()

        display(widgets.VBox([
            widgets.HTML("<h2>ATS Resume Score Checker</h2>"),
            self.progress,
            self.status,
            widgets.HTML("<h3>Results:</h3>"),
            self.score_display,
            widgets.HTML("<h3>Improvements:</h3>"),
            self.improvement_display
        ]))

    def update_status(self, value, message):
        self.progress.value = value
        self.status.value = f"<b>{message}</b>"
        time.sleep(0.2)

    def analyze(self, resume_file, jd_file=None):
        try:
            self.update_status(10, "Processing resume...")
            resume_text = extract_text(resume_file)

            jd_text = ""
            if jd_file:
                self.update_status(20, "Processing job description...")
                jd_text = extract_text(jd_file)

            self.update_status(40, "Analyzing content...")
            scanner = ATSScanner()
            score, sections = scanner.analyze(resume_text, jd_text)

            self.update_status(70, "Preparing results...")
            self.show_score(score)
            self.show_improvements(resume_text, jd_text, sections, scanner)

            self.update_status(100, "Analysis complete!")

        except Exception as e:
            self.status.value = f"<b style='color:red'>Error: {str(e)}</b>"

    def show_score(self, score):
        with self.score_display:
            self.score_display.clear_output()

            if score >= 80:
                color = "green"
                rating = "Excellent"
            elif score >= 60:
                color = "orange"
                rating = "Good"
            else:
                color = "red"
                rating = "Needs Work"

            fig, ax = plt.subplots(figsize=(8, 2))
            ax.barh(['ATS Score'], [score], color=color)
            ax.set_xlim(0, 100)
            ax.set_title(f'{rating} ({score}/100)')
            plt.show()

    def show_improvements(self, resume_text, jd_text, sections, scanner):
        with self.improvement_display:
            self.improvement_display.clear_output()

            missing = [s for s in ['experience', 'education', 'skills'] if s not in sections]
            if missing:
                print(" Missing critical sections:")
                print(f"   - Add: {', '.join(missing)}\n")

            if jd_text:
                print("Job Description Analysis:")

                resume_words = set(preprocess(resume_text))
                missing_keywords = [kw for kw in scanner.jd_keywords if kw not in resume_words]

                if missing_keywords:
                    print(f"   - Add these JD keywords: {', '.join(set(missing_keywords[:10]))}")

                jd_match = scanner.calculate_jd_match(resume_text, jd_text)
                print(f"   - JD Match: {int(jd_match*100)}% similarity\n")

            print(" General Improvements:")

            action_verbs = ['achieved', 'increased', 'optimized', 'developed', 'led']
            missing_verbs = [v for v in action_verbs if v not in resume_text.lower()]
            if missing_verbs:
                print(f"   - Use more action verbs like: {', '.join(missing_verbs)}")

            word_count = len(resume_text.split())
            if word_count > 800:
                print("   - Reduce length (aim for 1-2 pages)")
            elif word_count < 300:
                print("   - Add more details (too short)")

def main():
    print("=== ATS Resume Optimizer ===")
    print("1. Upload your resume (PDF/DOCX/TXT):")
    uploaded_resume = files.upload()
    if not uploaded_resume:
        print("No resume uploaded!")
        return

    resume_file = list(uploaded_resume.keys())[0]

    print("\n2. Optional: Upload job description (PDF/DOCX/TXT):")
    uploaded_jd = files.upload()
    jd_file = list(uploaded_jd.keys())[0] if uploaded_jd else None

    analyzer = ResumeAnalyzer()
    analyzer.analyze(resume_file, jd_file)

if __name__ == "__main__":
    main()